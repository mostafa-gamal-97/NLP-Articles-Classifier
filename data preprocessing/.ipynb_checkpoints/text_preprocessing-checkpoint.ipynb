{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6e6d155",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445c48c3",
   "metadata": {},
   "source": [
    "### Importing libraries and setting path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdf36611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: C:\\Users\\HP\\Desktop\\For CV\\Project 5\\preprocessing\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 23>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# change working directory and list all files in new directory\u001b[39;00m\n\u001b[0;32m     22\u001b[0m os\u001b[38;5;241m.\u001b[39mchdir(project_path)\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew working directory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mdata_path\u001b[49m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dirname, _, filenames \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mwalk(data_path):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_path' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "# text preprocessing\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Increase width to see the article_title clearly\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Our project path\n",
    "project_path = r\"C:\\Users\\HP\\Desktop\\For CV\\Project 5\"\n",
    "# check working directory\n",
    "print(\"Working directory:\",os.getcwd())\n",
    "# change working directory and list all files in new directory\n",
    "os.chdir(project_path)\n",
    "print(\"New working directory\", project_path)\n",
    "print()\n",
    "for dirname, _, filenames in os.walk(project_path):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c435da12",
   "metadata": {},
   "source": [
    "### Load our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a878659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\HP\\Desktop\\For CV\\Project 5\\data cleaning\\clean_df.csv\", encoding='ISO-8859-1')\n",
    "print(df.columns)\n",
    "# drop the Unnamed: 0' column\n",
    "df.drop(columns = 'Unnamed: 0', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc00b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())\n",
    "print()\n",
    "display(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b18f3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# value counts\n",
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44700cfa",
   "metadata": {},
   "source": [
    "### Train of thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3822de1d",
   "metadata": {},
   "source": [
    "We need to get our article_titles in dataframe to a state where it's ready to use by machine learning models.\n",
    "\n",
    "I will start by putting the steps to follow, and then writing **helper functions**(if needed) to perform these steps.\n",
    "\n",
    "Finally, I will put the whole process in one big function called **text_preprocessing** that takes a dataframe and returns a cleaned preprocessed one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d4a8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1: Remove punctation\n",
    "# step2: .lower()\n",
    "# step3: word tokenization(taking each sentence and splitting it into a list of words)\n",
    "# step4: Stopwords Removal\n",
    "# step5: Stemming \n",
    "# step6: Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e13bea",
   "metadata": {},
   "source": [
    "#### Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaa06e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    puncts_free = \"\".join([i for i in text if i not in string.punctuation])\n",
    "    return puncts_free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50678ac8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['puncts_free'] = df['article_title'].apply(lambda x: remove_punctuation(x))\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3df9dd",
   "metadata": {},
   "source": [
    "#### Lowering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94416bff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['puncts_free_lower'] = df['puncts_free'].apply(lambda x: x.lower())\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32644a8",
   "metadata": {},
   "source": [
    "#### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1514b0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can either use spacy or nltk, but nltk is better for our purposes here\n",
    "df['tokenized_words'] = df.apply(lambda x: word_tokenize(x['puncts_free_lower']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5025cdf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# testing\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35bb35e",
   "metadata": {},
   "source": [
    "We only need **'tokenized_words'**, and **'category'** columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7864c800",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df[['tokenized_words', 'category']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d6cef3",
   "metadata": {},
   "source": [
    "#### Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a11a562",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')\n",
    "print(stopwords, \" | \",len(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d508d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing our function\n",
    "def remove_stopwords(text_list):\n",
    "    stop_words_free = [i for i in text_list if i not in stopwords]\n",
    "    return stop_words_free    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a6100a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "df['sw_free'] = df['tokenized_words'].apply(lambda x: remove_stopwords(x))\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f09c8f7",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e359da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining stemming object\n",
    "pstemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d534cec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(text):\n",
    "    stemmed_text = [pstemmer.stem(word) for word in text]\n",
    "    return stemmed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d439d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing our function\n",
    "df['stemmed_title'] = df['sw_free'].apply(lambda x: stemming(x))\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65303def",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21f797d",
   "metadata": {},
   "source": [
    "Before starting the lemmatization process, open your cmd and:\n",
    "\n",
    "* type python\n",
    "* import nltk\n",
    "* nltk.download('omw-1.4')\n",
    "\n",
    "for more info:\n",
    "[stackoverflow wordnet](https://stackoverflow.com/questions/48152637/why-is-nltk-download-unable-to-download-wordnet-or-any-other-data)\n",
    "\n",
    "This will download the WordNet(nltk corpus reader).\n",
    "\n",
    "If not downloaded, it will throw an error when you try to use the **WordNetLemmatizer()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78df386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining lemmatization object\n",
    "wnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca3cb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(text):\n",
    "    lemmatized_text = [wnet_lemmatizer.lemmatize(word) for word in text]\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85768ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing our function\n",
    "df['lemmatized_title'] = df['stemmed_title'].apply(lambda x: lemmatization(x))\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6f1f0c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = df[['stemmed_title', 'lemmatized_title', 'category']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f40ac9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed text to a csv file\n",
    "df.to_csv(r\"C:\\Users\\HP\\Desktop\\For CV\\Project 5\\preprocessing\\final_df_1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f1231e",
   "metadata": {},
   "source": [
    "#### Final Preprocessed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7b6f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['final_text'] = df['lemmatized_title'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc29577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save preprocessed df\n",
    "pp_df = df[['final_text', 'category']]\n",
    "pp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e99502",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_df.to_csv(r\"C:\\Users\\HP\\Desktop\\For CV\\Project 5\\preprocessing\\final_df_2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c3ea40",
   "metadata": {},
   "source": [
    "### Final Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5585865",
   "metadata": {},
   "source": [
    "Our data is cleaned, preprocessed, and ready.\n",
    "\n",
    "In the next notebook, We will convert our text data to numeric form to be ready for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f122fa9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
